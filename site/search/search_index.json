{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to our ACI Automation Training with Practice Lab Speakers: Justyna Chowaniec , Consulting Engineer, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc. Cisco ACI Automation Cisco Application Centric Infrastructure (ACI) is a software-defined networking (SDN) solution designed for data centers. Cisco ACI allows network infrastructure to be defined based upon network policies \u2013 simplifying, optimizing, and accelerating the application deployment lifecycle. Network Controllers provide more than just a centralized management for networks today. They can also provide a place in the network to deploy network applications to add new capabilities. The Cisco ACI programmability model provides complete programmatic access. You can integrate network deployment with management and monitoring tools and programmatically deploy new workloads. Networks have traditionally been built with devices that are designed to be configured and maintained on a per device basis. To make changes or troubleshoot, a network engineer must connect to multiple devices individually and enter commands on the CLI. This solution works fine for mostly static environments, but does not scale and is prone to human error as changes are made more frequently and repetitively. Since the CLI was built for humans, it is less than ideal interface for programmability and automation. This is the core problem ACI solves for data center operations. ACI's programmability options are made possible through the ACI Object Model. ACI was built with programmability in mind and designed to be configured and maintained through a central controller via a REST API. This API is how admins interact with the object-model allowing them to create, make changes, gather stats, and troubleshoot the ACI fabric. During this ACI Automation practice training you will have a chance to explore ACI Object Model and learn how to leverage it for automation in daily operations. High Level Design of Lab scenario. ACI Automation Lab is based on dCloud session publicly available. Session name Started with Cisco ACI 5.2 v1 can be found in Session catalog at https://dcloud2-lon.cisco.com/ . Below is the physical setup of our Virtual lab. Enjoy!","title":"Home"},{"location":"#welcome-to-our-aci-automation-training-with-practice-lab","text":"Speakers: Justyna Chowaniec , Consulting Engineer, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc.","title":"Welcome to our ACI Automation Training with Practice Lab"},{"location":"#cisco-aci-automation","text":"Cisco Application Centric Infrastructure (ACI) is a software-defined networking (SDN) solution designed for data centers. Cisco ACI allows network infrastructure to be defined based upon network policies \u2013 simplifying, optimizing, and accelerating the application deployment lifecycle. Network Controllers provide more than just a centralized management for networks today. They can also provide a place in the network to deploy network applications to add new capabilities. The Cisco ACI programmability model provides complete programmatic access. You can integrate network deployment with management and monitoring tools and programmatically deploy new workloads. Networks have traditionally been built with devices that are designed to be configured and maintained on a per device basis. To make changes or troubleshoot, a network engineer must connect to multiple devices individually and enter commands on the CLI. This solution works fine for mostly static environments, but does not scale and is prone to human error as changes are made more frequently and repetitively. Since the CLI was built for humans, it is less than ideal interface for programmability and automation. This is the core problem ACI solves for data center operations. ACI's programmability options are made possible through the ACI Object Model. ACI was built with programmability in mind and designed to be configured and maintained through a central controller via a REST API. This API is how admins interact with the object-model allowing them to create, make changes, gather stats, and troubleshoot the ACI fabric. During this ACI Automation practice training you will have a chance to explore ACI Object Model and learn how to leverage it for automation in daily operations.","title":"Cisco ACI Automation"},{"location":"#high-level-design-of-lab-scenario","text":"ACI Automation Lab is based on dCloud session publicly available. Session name Started with Cisco ACI 5.2 v1 can be found in Session catalog at https://dcloud2-lon.cisco.com/ . Below is the physical setup of our Virtual lab. Enjoy!","title":"High Level Design of Lab scenario."},{"location":"LAB_access-RDP/","text":"Connectivity Check 1. Lab access general description The lab has been built leveraging multiple environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem. 2. VPN connection to dCloud infrastructure The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials. Access session with RDP Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client and use credentials provided below: IP Address: 198.18.133.10 Username: administrator User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. 3. Accessing Linux Jumphost Open PuTTY client on RDP-workstation taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345 4. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: Please connect to Cisco Intersight from RDP session. URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Tip Intersight username you find in POD details at WILAssistant page. User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. 5. Accessing Cisco Intersight Assist Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collects data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created. 5. Accessing CSR1kv Lab router Your session contains Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configurations are ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router. 6. Accessing vCenter for Lab Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"Connectivity Check"},{"location":"LAB_access-RDP/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access-RDP/#1-lab-access-general-description","text":"The lab has been built leveraging multiple environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem.","title":"1. Lab access general description"},{"location":"LAB_access-RDP/#2-vpn-connection-to-dcloud-infrastructure","text":"The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials.","title":"2. VPN connection to dCloud infrastructure"},{"location":"LAB_access-RDP/#access-session-with-rdp","text":"Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client and use credentials provided below: IP Address: 198.18.133.10 Username: administrator User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop.","title":"Access session with RDP"},{"location":"LAB_access-RDP/#3-accessing-linux-jumphost","text":"Open PuTTY client on RDP-workstation taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345","title":"3. Accessing Linux Jumphost"},{"location":"LAB_access-RDP/#4-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: Please connect to Cisco Intersight from RDP session. URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Tip Intersight username you find in POD details at WILAssistant page. User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"4. Accessing Cisco Intersight Platform"},{"location":"LAB_access-RDP/#5-accessing-cisco-intersight-assist","text":"Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collects data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created.","title":"5. Accessing Cisco Intersight Assist"},{"location":"LAB_access-RDP/#5-accessing-csr1kv-lab-router","text":"Your session contains Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configurations are ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router.","title":"5. Accessing CSR1kv Lab router"},{"location":"LAB_access-RDP/#6-accessing-vcenter-for-lab","text":"Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"6. Accessing vCenter for Lab"},{"location":"LAB_access/","text":"Connectivity Check 1. Lab access general description The lab is based on dCloud session Started with Cisco ACI 5.2 v1 : You will have access to Cisco dCloud User Interface, where you will be able to login to necessary resources. Each dCloud session contain Windows Workstation, Centos server tool and ACI Simulator. You will work on your dedicated session - details will be provided by instructor. During the training you will use only dCloud resources, no need to install additional applications at your PC. 2. Cisco dCloud dashboard The entire lab for the session is built using Cisco dCloud environment. Access to the Session will be provided by the proctor assigned to you. You will get credentials for your individual session provided by teacher. Access session with webRDP Once logged to the dCloud session, you will see dashboard like on the following picture: To open * webRDP follow the procedure from the figures: 1) Click on the blue triangle highlighted on figure below 2) Follow to \"Remote Desktop\" by using link in red frame on the figure: When you click on \"Remote Desktop\" link, your web browser will open new TAB with access to Windows desktop. The webRDP has installed basic tools to operate with the LAB. Rest of them you will install yourself for better undestanding of prerequisites for Automation. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win-workstation can be accessed that way. Tip When you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole. 3. Accessing Linux Machine Open PuTTY client on webRDP taskbar. PuTTY has predefined session called tool1 for centos-terminal. Open SSH session by selecting it and click Open button. Username: root User password: C1sco12345 4. Accessing ACI simulator The LAB is based on ACI simulator installed for you. To login, please use CHROME at webRDP workstation. Once you click at Chrome icon in taskbar, you get APIC gui open. Login using credentials below URL: https://apic1.dcloud.cisco.com/ Username: admin User password: C1sco12345 Warning Do not delete any configuration already present at ANY device within the LAB topology.","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#1-lab-access-general-description","text":"The lab is based on dCloud session Started with Cisco ACI 5.2 v1 : You will have access to Cisco dCloud User Interface, where you will be able to login to necessary resources. Each dCloud session contain Windows Workstation, Centos server tool and ACI Simulator. You will work on your dedicated session - details will be provided by instructor. During the training you will use only dCloud resources, no need to install additional applications at your PC.","title":"1. Lab access general description"},{"location":"LAB_access/#2-cisco-dcloud-dashboard","text":"The entire lab for the session is built using Cisco dCloud environment. Access to the Session will be provided by the proctor assigned to you. You will get credentials for your individual session provided by teacher.","title":"2. Cisco dCloud dashboard"},{"location":"LAB_access/#access-session-with-webrdp","text":"Once logged to the dCloud session, you will see dashboard like on the following picture: To open * webRDP follow the procedure from the figures: 1) Click on the blue triangle highlighted on figure below 2) Follow to \"Remote Desktop\" by using link in red frame on the figure: When you click on \"Remote Desktop\" link, your web browser will open new TAB with access to Windows desktop. The webRDP has installed basic tools to operate with the LAB. Rest of them you will install yourself for better undestanding of prerequisites for Automation. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win-workstation can be accessed that way. Tip When you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole.","title":"Access session with webRDP"},{"location":"LAB_access/#3-accessing-linux-machine","text":"Open PuTTY client on webRDP taskbar. PuTTY has predefined session called tool1 for centos-terminal. Open SSH session by selecting it and click Open button. Username: root User password: C1sco12345","title":"3. Accessing Linux Machine"},{"location":"LAB_access/#4-accessing-aci-simulator","text":"The LAB is based on ACI simulator installed for you. To login, please use CHROME at webRDP workstation. Once you click at Chrome icon in taskbar, you get APIC gui open. Login using credentials below URL: https://apic1.dcloud.cisco.com/ Username: admin User password: C1sco12345 Warning Do not delete any configuration already present at ANY device within the LAB topology.","title":"4. Accessing ACI simulator"},{"location":"backend_exercise/","text":"Explore Backend App and Kubernetes Dashboard: Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@CLUS-IKS-1 Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url http://<kubernetes node's external ip>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"backend_exercise/#explore-backend-app-and-kubernetes-dashboard","text":"Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@CLUS-IKS-1 Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url http://<kubernetes node's external ip>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name> Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name> Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name> Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name>","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name>","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name>","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication. 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects - 1.1 Create Kubernetes Secret for MariaDB: A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Kubernetes Persistent Volume Claim for MariaDB: A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed. 1.3 Deploy MariaDB on Kubernetes: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up. 1.4 Create Kubernetes LoadBalancer Service for MariaDB: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot 2. Deploy REST API Agent on Kubernetes: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS. 2.1 Deploy REST API Agent: The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> ' 2.2 Create Kubernetes NodePort Service for REST API Agent: Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot 2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 3. Deploy MQTT to DB Agent on Kubernetes: This part of deployment will be done in Public Cloud (AWS) K8s tenant. 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. Tip Copy following command to notepad FIRST, change IP address and then paste it to Terminal. Command to edit: sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods 4 Test the REST APIs Exposed by REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster)"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-iks-kubernetes-cluster-iks-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers","title":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster)"},{"location":"deploy_backend/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication.","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects -","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret-for-mariadb","text":"A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret for MariaDB:"},{"location":"deploy_backend/#12-create-kubernetes-persistent-volume-claim-for-mariadb","text":"A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed.","title":"1.2 Create Kubernetes Persistent Volume Claim for MariaDB:"},{"location":"deploy_backend/#13-deploy-mariadb-on-kubernetes","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up.","title":"1.3 Deploy MariaDB on Kubernetes:"},{"location":"deploy_backend/#14-create-kubernetes-loadbalancer-service-for-mariadb","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot","title":"1.4 Create Kubernetes LoadBalancer Service for MariaDB:"},{"location":"deploy_backend/#2-deploy-rest-api-agent-on-kubernetes","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS.","title":"2. Deploy REST API Agent on Kubernetes:"},{"location":"deploy_backend/#21-deploy-rest-api-agent","text":"The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> '","title":"2.1 Deploy REST API Agent:"},{"location":"deploy_backend/#22-create-kubernetes-nodeport-service-for-rest-api-agent","text":"Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot","title":"2.2 Create Kubernetes NodePort Service for REST API Agent:"},{"location":"deploy_backend/#23-locate-the-ip-and-port-to-access-node-port-service-for-rest-api-agent","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent:"},{"location":"deploy_backend/#3-deploy-mqtt-to-db-agent-on-kubernetes","text":"This part of deployment will be done in Public Cloud (AWS) K8s tenant. 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. Tip Copy following command to notepad FIRST, change IP address and then paste it to Terminal. Command to edit: sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods","title":"3. Deploy MQTT to DB Agent on Kubernetes:"},{"location":"deploy_backend/#4-test-the-rest-apis-exposed-by-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"4 Test the REST APIs Exposed by REST API Agent Service:"},{"location":"deploy_frontend-aws/","text":"Deploy the Frontend Application Components on AWS In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 1. Deploy frontend-iot: Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram 1.1 Create ConfigMap Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command - Copy it to notepad FIRST, edit and paste to Terminal: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT> 1.2 Create new deployment: iot-frontend Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:vegas2022-v5\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\" 2. Expose the Application by Creating Kubernetes Service: Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80. 2.1 Check status of newly created service Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. 3. Open the Application Dashboard: 3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#deploy-the-frontend-application-components-on-aws","text":"In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_frontend-aws/#1-deploy-frontend-iot","text":"Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram","title":"1. Deploy frontend-iot:"},{"location":"deploy_frontend-aws/#11-create-configmap","text":"Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command - Copy it to notepad FIRST, edit and paste to Terminal: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT>","title":"1.1 Create ConfigMap"},{"location":"deploy_frontend-aws/#12-create-new-deployment-iot-frontend","text":"Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:vegas2022-v5\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\"","title":"1.2 Create new deployment: iot-frontend"},{"location":"deploy_frontend-aws/#2-expose-the-application-by-creating-kubernetes-service","text":"Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80.","title":"2. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend-aws/#21-check-status-of-newly-created-service","text":"Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet.","title":"2.1 Check status of newly created service"},{"location":"deploy_frontend-aws/#3-open-the-application-dashboard","text":"3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"3. Open the Application Dashboard:"},{"location":"guacamole/","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"dCloudGuacamole"},{"location":"guacamole/#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole/#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"intersight/","text":"Explore Cisco Intersight Dashboard 1. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. Access Intersight using following steps: Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help. 2. Intersigh Dashboard Target While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target. 3. Intersight Dashboard Operate Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes. Virtualization When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested. Kuberenetes Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case. Explore deployed IKS cluster Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them. 4. Intersight Dashboard Configure Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored. IKS Profile wizzard When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT. IKS Configure Polices Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#explore-cisco-intersight-dashboard","text":"","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#1-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"1. Accessing Cisco Intersight Platform"},{"location":"intersight/#access-intersight-using-following-steps","text":"Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help.","title":"Access Intersight using following steps:"},{"location":"intersight/#2-intersigh-dashboard-target","text":"While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target.","title":"2. Intersigh Dashboard Target"},{"location":"intersight/#3-intersight-dashboard-operate","text":"Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes.","title":"3. Intersight Dashboard Operate"},{"location":"intersight/#virtualization","text":"When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested.","title":"Virtualization"},{"location":"intersight/#kuberenetes","text":"Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case.","title":"Kuberenetes"},{"location":"intersight/#explore-deployed-iks-cluster","text":"Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them.","title":"Explore deployed IKS cluster"},{"location":"intersight/#4-intersight-dashboard-configure","text":"Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored.","title":"4. Intersight Dashboard Configure"},{"location":"intersight/#iks-profile-wizzard","text":"When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT.","title":"IKS Profile wizzard"},{"location":"intersight/#iks-configure-polices","text":"Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"IKS Configure Polices"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID> Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID>","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"},{"location":"monitor-aci/","text":"How to pull informations using restAPI blablabla 1. first section 1.1 sub-section 2. Second section","title":"How to pull informations using restAPI"},{"location":"monitor-aci/#how-to-pull-informations-using-restapi","text":"blablabla","title":"How to pull informations using restAPI"},{"location":"monitor-aci/#1-first-section","text":"","title":"1. first section"},{"location":"monitor-aci/#11-sub-section","text":"","title":"1.1 sub-section"},{"location":"monitor-aci/#2-second-section","text":"","title":"2. Second section"},{"location":"postman/","text":"Install and Operate Postman Postman is an API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration so you can create better APIs\u2014faster. 1. Install Postman in your workstation First step to do in our dCloud lab is download and install of Postman. Login to your dCloud workstation and from the Chrome connect to: https://www.postman.com/downloads/ Select Windows 64-bit to download the installation file. Download the file to local folder of your choice. Start installation - it will automatically install and open application. Note You installed and run Postman in Scratch Pad version. If you would like to use workspace version, it is required to setup Free account. It will keep your Postman Collections in cloud, helps to collaborate and backup work you do. 2 Initial setup of Postman Once the Postman is up and running you should create few things. First, setup your Environment data. Environment definition can be reused later, it is also good way to keep secured sensitive data like credentials. Second, create your Request Collection or import one from .json file. 2.1 Create Environment for our dCloud lab Environment definition is a place where you can store information about IP address of APIC and credentials. Once you run your restAPI queries, Postman will automatically use variables defined here. Lets do it then for our scenario. Navigate to Environment section in Postman Dashboard as marked on figure below. In this place you can select existing Environment, edit/delete it or create new one. To review existing Environment use icon most on right in red marked section on Figure above. It will open section shown at next figure. Click Add to create new entry. Add button will move you to new Postman Tab Change name of your Environment to: ACI-dcloud Configure VARIABLES: apic, user, password INITIAL VALUES: 198.18.133.200, admin, C1sco12345 Change Type of data for password to secret Save it. Now you have ready Environment you can choice from drop-down menu. You will use it for rest of Day1 exercises. 2.2 Create New Collection Click on Link Create Collection indicated in the figure below: Name your New Collection ACI dCloud Set Authorization type to Basic Auth . Once done, use Ctrl+S or Save button on dashboard - Floppy Disk icon. Do not type anything to \"Username\" and \"Password\". Those data will be pulled from your Environment - created in previous task. Now you have Environment and Collection ready. We can start working with our restAPI requests.","title":"Install and operate Postman"},{"location":"postman/#install-and-operate-postman","text":"Postman is an API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration so you can create better APIs\u2014faster.","title":"Install and Operate Postman"},{"location":"postman/#1-install-postman-in-your-workstation","text":"First step to do in our dCloud lab is download and install of Postman. Login to your dCloud workstation and from the Chrome connect to: https://www.postman.com/downloads/ Select Windows 64-bit to download the installation file. Download the file to local folder of your choice. Start installation - it will automatically install and open application. Note You installed and run Postman in Scratch Pad version. If you would like to use workspace version, it is required to setup Free account. It will keep your Postman Collections in cloud, helps to collaborate and backup work you do.","title":"1. Install Postman in your workstation"},{"location":"postman/#2-initial-setup-of-postman","text":"Once the Postman is up and running you should create few things. First, setup your Environment data. Environment definition can be reused later, it is also good way to keep secured sensitive data like credentials. Second, create your Request Collection or import one from .json file.","title":"2 Initial setup of Postman"},{"location":"postman/#21-create-environment-for-our-dcloud-lab","text":"Environment definition is a place where you can store information about IP address of APIC and credentials. Once you run your restAPI queries, Postman will automatically use variables defined here. Lets do it then for our scenario. Navigate to Environment section in Postman Dashboard as marked on figure below. In this place you can select existing Environment, edit/delete it or create new one. To review existing Environment use icon most on right in red marked section on Figure above. It will open section shown at next figure. Click Add to create new entry. Add button will move you to new Postman Tab Change name of your Environment to: ACI-dcloud Configure VARIABLES: apic, user, password INITIAL VALUES: 198.18.133.200, admin, C1sco12345 Change Type of data for password to secret Save it. Now you have ready Environment you can choice from drop-down menu. You will use it for rest of Day1 exercises.","title":"2.1 Create Environment for our dCloud lab"},{"location":"postman/#22-create-new-collection","text":"Click on Link Create Collection indicated in the figure below: Name your New Collection ACI dCloud Set Authorization type to Basic Auth . Once done, use Ctrl+S or Save button on dashboard - Floppy Disk icon. Do not type anything to \"Username\" and \"Password\". Those data will be pulled from your Environment - created in previous task. Now you have Environment and Collection ready. We can start working with our restAPI requests.","title":"2.2 Create New Collection"},{"location":"resources/","text":"Resources from the LAB In this section you can find necessary files used during the Lab to download. Cisco API Documentation Cisco ACI API Configuration guide Cisco ACI Authentication API CSV Files used for deployments tenant-create.csv - used for deployment of 22 BDs/EPGs in new Tenant tenant-common.csv - used for use-cases, configuration of Common Tenant var-data-uc1 - used for Use-Case1 custom tenant definition JSON additional files to deploy ACI configuration Fabric Switches Provisioning JSON interface leaf profile JSON switch leaf profile JSON switch VPC policy JSON Tenant-1 Ready Postman Environment and Collection - FULL LIST ACI-dcloud Environment ACI dCloud Collection","title":"Resources"},{"location":"resources/#resources-from-the-lab","text":"In this section you can find necessary files used during the Lab to download.","title":"Resources from the LAB"},{"location":"resources/#cisco-api-documentation","text":"Cisco ACI API Configuration guide Cisco ACI Authentication API","title":"Cisco API Documentation"},{"location":"resources/#csv-files-used-for-deployments","text":"tenant-create.csv - used for deployment of 22 BDs/EPGs in new Tenant tenant-common.csv - used for use-cases, configuration of Common Tenant var-data-uc1 - used for Use-Case1 custom tenant definition","title":"CSV Files used for deployments"},{"location":"resources/#json-additional-files-to-deploy-aci-configuration","text":"Fabric Switches Provisioning JSON interface leaf profile JSON switch leaf profile JSON switch VPC policy JSON Tenant-1 Ready","title":"JSON additional files to deploy ACI configuration"},{"location":"resources/#postman-environment-and-collection-full-list","text":"ACI-dcloud Environment ACI dCloud Collection","title":"Postman Environment and Collection - FULL LIST"},{"location":"restAPI/","text":"Cisco ACI rest API In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined. 1 Define restAPI calls under Collection Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call: 1.1 Create ACI login request First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad. Register your ACI Fabric Switches in APIC Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues. 1.2 Get Information About a Node POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again. 2 ACI Access Polices Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests. 2.1 Interface Policies You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } } 2.2 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } } 2.3 Interface Policy Group Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download two files to your dcloud workstation and using post in APIC GUI upload it to access-polices. Without, your VPC won't instanciate and cannot be used in later stage of lab. JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear. 3 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above. 3.1 Tenant components This section contain JSON codes necessary to create Tenant objects. 3.1.1 Tenant and VRF Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } } 3.1.2 BD in existing tenant Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain. 3.1.3 Application Profile Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } } 3.1.4 EPGs in existing tenant/appprofiles and associated with domain https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } 4 Use CSV file for input data You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run. 4.1 Run Collection requests Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button. 4.2 Verification after runing the collection When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"Automation basis with restAPI"},{"location":"restAPI/#cisco-aci-rest-api","text":"In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined.","title":"Cisco ACI rest API"},{"location":"restAPI/#1-define-restapi-calls-under-collection","text":"Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call:","title":"1 Define restAPI calls under Collection"},{"location":"restAPI/#11-create-aci-login-request","text":"First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad.","title":"1.1 Create ACI login request"},{"location":"restAPI/#register-your-aci-fabric-switches-in-apic","text":"Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues.","title":"Register your ACI Fabric Switches in APIC"},{"location":"restAPI/#12-get-information-about-a-node","text":"POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again.","title":"1.2 Get Information About a Node"},{"location":"restAPI/#2-aci-access-polices","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests.","title":"2  ACI Access Polices"},{"location":"restAPI/#21-interface-policies","text":"You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } }","title":"2.1 Interface Policies"},{"location":"restAPI/#22-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } }","title":"2.2 VLANs, Domains and AAEPs"},{"location":"restAPI/#23-interface-policy-group","text":"Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download two files to your dcloud workstation and using post in APIC GUI upload it to access-polices. Without, your VPC won't instanciate and cannot be used in later stage of lab. JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear.","title":"2.3 Interface Policy Group"},{"location":"restAPI/#3-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above.","title":"3  ACI Tenant"},{"location":"restAPI/#31-tenant-components","text":"This section contain JSON codes necessary to create Tenant objects.","title":"3.1 Tenant components"},{"location":"restAPI/#311-tenant-and-vrf","text":"Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } }","title":"3.1.1 Tenant and VRF"},{"location":"restAPI/#312-bd-in-existing-tenant","text":"Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain.","title":"3.1.2 BD in existing tenant"},{"location":"restAPI/#313-application-profile","text":"Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } }","title":"3.1.3 Application Profile"},{"location":"restAPI/#314-epgs-in-existing-tenantappprofiles-and-associated-with-domain","text":"https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } }","title":"3.1.4 EPGs in existing tenant/appprofiles and associated with domain"},{"location":"restAPI/#4-use-csv-file-for-input-data","text":"You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run.","title":"4 Use CSV file for input data"},{"location":"restAPI/#41-run-collection-requests","text":"Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button.","title":"4.1 Run Collection requests"},{"location":"restAPI/#42-verification-after-runing-the-collection","text":"When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"4.2 Verification after runing the collection"},{"location":"security_policies/","text":"Applying Kubernetes Network Policies to Secure the Application A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050'). 1. Apply Deny All Network Policy: Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working. 2. Apply Permit Port 5111 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why? 3. Apply Permit Port 5050 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#applying-kubernetes-network-policies-to-secure-the-application","text":"A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050').","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#1-apply-deny-all-network-policy","text":"Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working.","title":"1. Apply Deny All Network Policy:"},{"location":"security_policies/#2-apply-permit-port-5111-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why?","title":"2. Apply Permit Port 5111 Network Policy:"},{"location":"security_policies/#3-apply-permit-port-5050-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"3. Apply Permit Port 5050 Network Policy:"},{"location":"terraform-basis/","text":"Terraform setup HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features. 1. Install Teraform in your workstation Note Participants can use virtual machine with Windows or work on personal laptop. Please follow installation instructions for the OS on your laptops. To use Terraform you will need to install it. HashiCorp distributes Terraform as a binary package. You can also install Terraform using popular package managers. 1.1 For CentOS/RHEL distribution: Install yum-config-manager to manage your repositories. sudo yum install -y yum-utils Use yum-config-manager to add the official HashiCorp Linux repository. sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo Install Terraform from the new repository. sudo yum -y install terraform 1.2. For Windows OS: To install Terraform, find the appropriate package for your system and download it as a zip archive: https://releases.hashicorp.com/terraform/1.3.3/terraform_1.3.3_windows_amd64.zip After downloading Terraform, unzip the package. Terraform runs as a single binary named terraform. Any other files in the package can be safely removed and Terraform will still function. Copy the terraform.bin to C:\\Windows folder. Finally, make sure that the terraform binary is available on your PATH. The path can be edited through: 1) Find Advanced system settings -> Advanced -> Environment Variables 2) Find PATH variable and click edit to change 3) Verify if C:\\Windows exists in the path and if not add it to the list 1.3 For Macbook with OS X: Homebrew is a free and open-source package management system for Mac OS X. Install the official Terraform formula from the terminal. First, install the HashiCorp tap, a repository of all our Homebrew packages. brew tap hashicorp/tap Now, install Terraform with hashicorp/tap/terraform. brew install hashicorp/tap/terraform Note This installs a signed binary and is automatically updated with every new official release. To update to the latest version of Terraform, first update Homebrew. brew update Then, run the upgrade command to download and use the latest Terraform version. brew upgrade hashicorp/tap/terraform ==> Upgrading 1 outdated package: hashicorp/tap/terraform 0.15.3 -> 1.0.0 ==> Upgrading hashicorp/tap/terraform 0.15.3 -> 1.0.0 1.3 Verify the installation Verify that the installation worked by opening a new terminal session and listing Terraform's available subcommands. terraform -help Usage: terraform [-version] [-help] <command> [args] The available commands for execution are listed below. The most common, useful commands are shown first, followed by less common or more advanced commands. If you're just getting started with Terraform, stick with the common commands. For the other commands, please read the help and docs before usage. #... Add any subcommand to terraform -help to learn more about what it does and available options. terraform -help plan","title":"Terraform setup"},{"location":"terraform-basis/#terraform-setup","text":"HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.","title":"Terraform setup"},{"location":"terraform-basis/#1-install-teraform-in-your-workstation","text":"Note Participants can use virtual machine with Windows or work on personal laptop. Please follow installation instructions for the OS on your laptops. To use Terraform you will need to install it. HashiCorp distributes Terraform as a binary package. You can also install Terraform using popular package managers.","title":"1. Install Teraform in your workstation"},{"location":"terraform-basis/#11-for-centosrhel-distribution","text":"Install yum-config-manager to manage your repositories. sudo yum install -y yum-utils Use yum-config-manager to add the official HashiCorp Linux repository. sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo Install Terraform from the new repository. sudo yum -y install terraform","title":"1.1 For CentOS/RHEL distribution:"},{"location":"terraform-basis/#12-for-windows-os","text":"To install Terraform, find the appropriate package for your system and download it as a zip archive: https://releases.hashicorp.com/terraform/1.3.3/terraform_1.3.3_windows_amd64.zip After downloading Terraform, unzip the package. Terraform runs as a single binary named terraform. Any other files in the package can be safely removed and Terraform will still function. Copy the terraform.bin to C:\\Windows folder. Finally, make sure that the terraform binary is available on your PATH. The path can be edited through: 1) Find Advanced system settings -> Advanced -> Environment Variables 2) Find PATH variable and click edit to change 3) Verify if C:\\Windows exists in the path and if not add it to the list","title":"1.2. For Windows OS:"},{"location":"terraform-basis/#13-for-macbook-with-os-x","text":"Homebrew is a free and open-source package management system for Mac OS X. Install the official Terraform formula from the terminal. First, install the HashiCorp tap, a repository of all our Homebrew packages. brew tap hashicorp/tap Now, install Terraform with hashicorp/tap/terraform. brew install hashicorp/tap/terraform Note This installs a signed binary and is automatically updated with every new official release. To update to the latest version of Terraform, first update Homebrew. brew update Then, run the upgrade command to download and use the latest Terraform version. brew upgrade hashicorp/tap/terraform ==> Upgrading 1 outdated package: hashicorp/tap/terraform 0.15.3 -> 1.0.0 ==> Upgrading hashicorp/tap/terraform 0.15.3 -> 1.0.0","title":"1.3 For Macbook with OS X:"},{"location":"terraform-basis/#13-verify-the-installation","text":"Verify that the installation worked by opening a new terminal session and listing Terraform's available subcommands. terraform -help Usage: terraform [-version] [-help] <command> [args] The available commands for execution are listed below. The most common, useful commands are shown first, followed by less common or more advanced commands. If you're just getting started with Terraform, stick with the common commands. For the other commands, please read the help and docs before usage. #... Add any subcommand to terraform -help to learn more about what it does and available options. terraform -help plan","title":"1.3 Verify the installation"},{"location":"terraform-usecases/","text":"Customer use-cases with Terraform In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI. 1. Install Visual Studio Code Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: - The left pane with the files - The right pane with the file contents - The bottom pane will be leveraging the Terminal to issue commands 1.1 Open a new terminal in the IDE To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-terminal.png\" width = 800> Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files. 2 Create the working directory Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-new-folder.png\" width = 800> <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-new-folder2.png\" width = 800> In that directory create the first terraform file called access_policies.tf 2. Second section","title":"Customer use-cases with Terraform"},{"location":"terraform-usecases/#customer-use-cases-with-terraform","text":"In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI.","title":"Customer use-cases with Terraform"},{"location":"terraform-usecases/#1-install-visual-studio-code","text":"Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: - The left pane with the files - The right pane with the file contents - The bottom pane will be leveraging the Terminal to issue commands","title":"1. Install Visual Studio Code"},{"location":"terraform-usecases/#11-open-a-new-terminal-in-the-ide","text":"To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-terminal.png\" width = 800> Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files.","title":"1.1 Open a new terminal in the IDE"},{"location":"terraform-usecases/#2-create-the-working-directory","text":"Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-new-folder.png\" width = 800> <img src=\"https://raw.githubusercontent.com/marcinduma/ACI-Automation/main/images/visual-new-folder2.png\" width = 800> In that directory create the first terraform file called access_policies.tf","title":"2 Create the working directory"},{"location":"terraform-usecases/#2-second-section","text":"","title":"2. Second section"},{"location":"use-cases-postman/","text":"Create sample use-cases with Postman When you think about automation, you should start with standardization. Definition of Use-Cases, Naming Convention, and AccessPolicies strategy is a minimum to start with. Once those three points are confirmed, you easily define your automation scripts. For use of our training we will work at three use-cases which very often appears in Customer's designs. Prerequisites - Tenant Common Configuration in Common Tenant is done once at initial config and then reused across EPGs. For that reason, your use-case should contain only Custom Tenant configuration. Configuration of Common Tenant can be deployed using already done Postman Requests. Prerequisite, you need to deploy, two VRFs, two Bridge-Domains and EPG_Shared. Use prepared CSV file for you - download from here . Run same collection which in previous excercise: Run ACI dCloud - results should be 200 OK . Use-Case no.1 Customer place network components in ACI shared tenant common . Custom tenants contain only EPGs, Domain associations and static-bindings for particular department. Below you can find code for Tenant Custom in use case 1. https://{{apic}}/api/node/mo/uni.json Tenant Custom Use-Case 1 { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" } } }, { \"fvRsBd\" : { \"attributes\" : { \"annotation\" : \"\" , \"tnFvBDName\" : \"{{bdname}}\" } } } ] } } ] } } ] } } Run the code with new CSV file which you will use for custom-tenant definition, together with static biding associated to EPGs download from here . Tenant Custom Use-Case 1 static biding { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsPathAtt\" : { \"attributes\" : { \"encap\" : \"vlan-{{vlan}}\" , \"instrImedcy\" : \"lazy\" , \"mode\" : \"{{mode}}\" , \"primaryEncap\" : \"unknown\" , \"tDn\" : \"topology/pod-{{pod}}/paths-{{leaf}}/pathep-[eth{{interface}}]\" , \"userdom\" : \":all:\" } } } ] } } ] } } ] } } Tip Assumption that Access-Policies, meaning VLAN pool and Domain Association is done beforehand. Otherwise use other Postman Requests to create proper vlan mapping Use-Case no.2 Customer place network components in ACI shared tenant common as well as Custom Tenant. Moreover custom tenants contain EPGs, Domain associations and static-bindings for particular department. Use-Case no.3 Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks.","title":"Create customer use-cases with Postman"},{"location":"use-cases-postman/#create-sample-use-cases-with-postman","text":"When you think about automation, you should start with standardization. Definition of Use-Cases, Naming Convention, and AccessPolicies strategy is a minimum to start with. Once those three points are confirmed, you easily define your automation scripts. For use of our training we will work at three use-cases which very often appears in Customer's designs.","title":"Create sample use-cases with Postman"},{"location":"use-cases-postman/#prerequisites-tenant-common","text":"Configuration in Common Tenant is done once at initial config and then reused across EPGs. For that reason, your use-case should contain only Custom Tenant configuration. Configuration of Common Tenant can be deployed using already done Postman Requests. Prerequisite, you need to deploy, two VRFs, two Bridge-Domains and EPG_Shared. Use prepared CSV file for you - download from here . Run same collection which in previous excercise: Run ACI dCloud - results should be 200 OK .","title":"Prerequisites - Tenant Common"},{"location":"use-cases-postman/#use-case-no1","text":"Customer place network components in ACI shared tenant common . Custom tenants contain only EPGs, Domain associations and static-bindings for particular department. Below you can find code for Tenant Custom in use case 1. https://{{apic}}/api/node/mo/uni.json Tenant Custom Use-Case 1 { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" } } }, { \"fvRsBd\" : { \"attributes\" : { \"annotation\" : \"\" , \"tnFvBDName\" : \"{{bdname}}\" } } } ] } } ] } } ] } } Run the code with new CSV file which you will use for custom-tenant definition, together with static biding associated to EPGs download from here . Tenant Custom Use-Case 1 static biding { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsPathAtt\" : { \"attributes\" : { \"encap\" : \"vlan-{{vlan}}\" , \"instrImedcy\" : \"lazy\" , \"mode\" : \"{{mode}}\" , \"primaryEncap\" : \"unknown\" , \"tDn\" : \"topology/pod-{{pod}}/paths-{{leaf}}/pathep-[eth{{interface}}]\" , \"userdom\" : \":all:\" } } } ] } } ] } } ] } } Tip Assumption that Access-Policies, meaning VLAN pool and Domain Association is done beforehand. Otherwise use other Postman Requests to create proper vlan mapping","title":"Use-Case no.1"},{"location":"use-cases-postman/#use-case-no2","text":"Customer place network components in ACI shared tenant common as well as Custom Tenant. Moreover custom tenants contain EPGs, Domain associations and static-bindings for particular department.","title":"Use-Case no.2"},{"location":"use-cases-postman/#use-case-no3","text":"Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks.","title":"Use-Case no.3"},{"location":"Cisco-ACI-PostMan-master/","text":"Cisco-ACI-PostMan Postman is a collaboration platform for API development. You can use Postman to design, build, and test APIs in conjunction with your teammates, and to support developer adoption. You can find more information and basic training resources for Postman here:- https://learning.postman.com/docs/postman/launching-postman/introduction/ In order to setup postman for ACI you can follow the links below:- https://community.cisco.com/t5/data-center-documents/aci-automation-part-1-aci-with-postman-introduction/ta-p/3318219 https://community.cisco.com/t5/data-center-documents/aci-automation-part-2-aci-with-postman-configuring-single-epg/ta-p/3318255 https://community.cisco.com/t5/data-center-documents/aci-automation-part-2-aci-with-postman-configuring-single-epg/ta-p/3318255 Postman collection for Cisco ACI How to use it? Import the collection and environment to your postman client. A sample environment (ACI Lab.postman_environment.json) is included for you reference where you can define the url for APIC and the username and password. Following high level tasks are included in the collections:- Basic APIC Configuration APIC Network Configuration APIC VMM Configuration Access Interface Policies Daily Operations Commands ( Read Only Commands to help you with operations, using GET) Runner Tasks ( Multiple Interface Static Path Binding) A sample CSV file has been uploaded for the runner tasks.","title":"Index"},{"location":"Cisco-ACI-PostMan-master/#cisco-aci-postman","text":"Postman is a collaboration platform for API development. You can use Postman to design, build, and test APIs in conjunction with your teammates, and to support developer adoption. You can find more information and basic training resources for Postman here:- https://learning.postman.com/docs/postman/launching-postman/introduction/ In order to setup postman for ACI you can follow the links below:- https://community.cisco.com/t5/data-center-documents/aci-automation-part-1-aci-with-postman-introduction/ta-p/3318219 https://community.cisco.com/t5/data-center-documents/aci-automation-part-2-aci-with-postman-configuring-single-epg/ta-p/3318255 https://community.cisco.com/t5/data-center-documents/aci-automation-part-2-aci-with-postman-configuring-single-epg/ta-p/3318255","title":"Cisco-ACI-PostMan"},{"location":"Cisco-ACI-PostMan-master/#postman-collection-for-cisco-aci","text":"","title":"Postman collection for Cisco ACI"},{"location":"Cisco-ACI-PostMan-master/#how-to-use-it","text":"Import the collection and environment to your postman client. A sample environment (ACI Lab.postman_environment.json) is included for you reference where you can define the url for APIC and the username and password. Following high level tasks are included in the collections:- Basic APIC Configuration APIC Network Configuration APIC VMM Configuration Access Interface Policies Daily Operations Commands ( Read Only Commands to help you with operations, using GET) Runner Tasks ( Multiple Interface Static Path Binding) A sample CSV file has been uploaded for the runner tasks.","title":"How to use it?"}]}